{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Train model with SageMaker</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create a Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from time import strftime, gmtime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set a sagemaker role  \n",
    "try:\n",
    "    # if you are on a sagemaker notebook instance\n",
    "    import sagemaker\n",
    "    role = sagemaker.get_execution_role()\n",
    "except: \n",
    "    # if locally, create a Sagemaker execution role in the aws console and assign it here\n",
    "    iam = boto3.client('iam')\n",
    "    role_name = \"YOUR SAGEMAKER ROLE\"\n",
    "    role = iam.get_role(RoleName=role_name)['Role']['Arn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using a local notabook, please make sure to modify `role_name` by a proper value. <br>\n",
    "For more details about roles, please sign in to [AWS Management Console](https://console.aws.amazon.com/iam/) and create a role in the left navigation pane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role: arn:aws:iam::612233423258:role/service-role/AmazonSageMaker-ExecutionRole-20200324T172595\n",
      "region: eu-central-1\n",
      "account: 612233423258\n"
     ]
    }
   ],
   "source": [
    "region = boto3.Session().region_name # get the region name\n",
    "account = boto3.Session().client('sts').get_caller_identity()['Account'] # get the account id\n",
    "sm = boto3.Session().client('sagemaker') # create a sagemaker session\n",
    "print(\"role: {}\".format(role))\n",
    "print(\"region: {}\".format(region))\n",
    "print(\"account: {}\".format(account))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specify the data and model location** <br>\n",
    "Please change the parameters in the following cells according to the location of your data and where you want to store the model artefacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data location : s3://sm-transformers-datasets/data/dataset_multiclass_500\n",
      "Models data location : s3://sm-transformers-datasets/models\n"
     ]
    }
   ],
   "source": [
    "# Data location\n",
    "bucket_name = \"sm-transformers-datasets\" # Bucket name where the data is located\n",
    "train_prefix = \"data/dataset_multiclass_500\" # folder of train data\n",
    "models_prefix = \"models\" # folder where model will be saved\n",
    "train_s3_uri = \"s3://{}/{}\".format(bucket_name, train_prefix)\n",
    "models_s3_uri = \"s3://{}/{}\".format(bucket_name, models_prefix)\n",
    "print(\"Train data location : {}\".format(train_s3_uri))\n",
    "print(\"Models data location : {}\".format(models_s3_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specify the docker image name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image of model: 612233423258.dkr.ecr.eu-central-1.amazonaws.com/sm-transformers-gpu:latest\n"
     ]
    }
   ],
   "source": [
    "image_name = \"sm-transformers-gpu\"\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(account, region, image_name)\n",
    "print(\"image of model: {}\".format(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set the training job name**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to explicitly set the training job name, ignore the following cell and change the value of `training_job_name` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training job name : sm-transformers-gpu-2021-02-17-09-55-35\n"
     ]
    }
   ],
   "source": [
    "# Training job name \n",
    "training_job_name = \"{}-{}\".format(image_name, strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime()))\n",
    "# shorten it (should be less than 63 characters)\n",
    "if len(training_job_name) > 63:\n",
    "    training_job_name = training_job_name[max(len(training_job_name)-62,0):]\n",
    "print(\"training job name : {}\".format(training_job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set checkpoints path**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally: You can specify an old training job name to be resumed !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints will be saved in s3://sm-transformers-datasets/models/sm-transformers-gpu-2021-02-17-09-55-35/checkpoints\n"
     ]
    }
   ],
   "source": [
    "checkpoints_s3_uri = \"s3://{}/{}/{}/checkpoints\".format(bucket_name, models_prefix, training_job_name) #old_training_job\n",
    "print(\"checkpoints will be saved in {}\".format(checkpoints_s3_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Name': 'training:epoch', 'Regex': \"'epoch': (.*?)}\"},\n",
       " {'Name': 'evaluation:loss', 'Regex': \"'eval_loss': (.*?),\"},\n",
       " {'Name': 'evaluation:accuracy', 'Regex': \"'eval_accuracy': (.*?),\"}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Metrics to follow during training (by parsing the logs!)\n",
    "metrics = [\n",
    "            {\n",
    "            \"Name\": \"training:epoch\",\n",
    "            \"Regex\": \"'epoch': (.*?)}\"\n",
    "            },\n",
    "            {\n",
    "            \"Name\": \"evaluation:loss\",\n",
    "            \"Regex\": \"'eval_loss': (.*?),\"\n",
    "            },\n",
    "            {\n",
    "            \"Name\": \"evaluation:accuracy\",\n",
    "            \"Regex\": \"'eval_accuracy': (.*?),\" # eval_mse(regression), eval_accuracy (classif), eval_accuracy_score(ner)\n",
    "            }\n",
    "        ]\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can adjust the hyperparametrs of the model/expand them. <br>\n",
    "For example, you can decrease batch size if you have OOM errors, increase/decrease the max sequence length, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## List of hyperparameters during training (optional)\n",
    "hyperparameters = {\n",
    "    \"task_name\": \"classif\",\n",
    "    \"model_name\": \"bert-base-uncased\",\n",
    "    \"max_steps\": \"500\",\n",
    "    \"use_bbox\": \"false\",\n",
    "    \"per_device_train_batch_size\": \"10\",\n",
    "    \"per_device_eval_batch_size\": \"10\"\n",
    "}\n",
    "#allenai/longformer-base-4096\n",
    "#bert-base-uncased\n",
    "#microsoft/layoutlm-base-uncased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick an instance type for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU : ml.g4dn.xlarge (ml.g4dn.xlarge   cpu:4     gpu:1xT4     cpu-ram:16    gpu-ram:16         training/hour$0.822)\n",
    "## classif: \n",
    "# bert : batch = 10 is ok (with text > 512) (70% GPU RAM busy)\n",
    "# Longformer: batch = 2 ok if text size after tokenization is < 2048 / batch 1: ok till limit! (4096) (89% GPU RAM utilised)\n",
    "\n",
    "## Token classif (ner)\n",
    "# bert: batch 10 is ok (with text > 512) (70% GPU RAM busy)\n",
    "# longformer: idem classif : batch = 2 ok if text size after tokenization is < 2048 / batch 1: ok till limit! (4096) (89% GPU RAM utilised)\n",
    "# layoutlm: batch 10 same than bert : is ok (with text > 512) (70% GPU RAM busy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## List of GPU instance to be chosen\n",
    "#name            CPUs   GPU     RAM  GPU-RAM  TrainingPrice/hour\n",
    "#ml.p3.2xlarge    8    1xV100    61    16         $4.627         \n",
    "#ml.p2.xlarge     4     1xK80    61    12         $1.361\n",
    "#ml.g4dn.xlarge   4     1xT4     16    16         $0.822\n",
    "#ml.g4dn.2xlarge  8     1xT4     32    16         $1.173  <-\n",
    "#ml.g4dn.4xlarge  16    1xT4     64    16         $1.879\n",
    "#ml.g4dn.8xlarge  32    1xT4     128   16         $3.396\n",
    "#ml.g4dn.12xlarge 48    4xT4     192   64         $6.107\n",
    "#ml.g4dn.16xlarge 64    1xT4     256   16         $6.794\n",
    "\n",
    "instance_type = \"ml.g4dn.xlarge\" # \"ml.c4.4xlarge\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify some additional parameters for the training job:\n",
    "- Training image\n",
    "- Arn Role\n",
    "- Model Location\n",
    "- Instance type for the training job\n",
    "- Data config:\n",
    "    - Location for the training data (and potentially test data if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"TrainingJobName\": \"sm-transformers-gpu-2021-02-17-09-55-35\",\n",
      "    \"AlgorithmSpecification\": {\n",
      "        \"TrainingImage\": \"612233423258.dkr.ecr.eu-central-1.amazonaws.com/sm-transformers-gpu:latest\",\n",
      "        \"TrainingInputMode\": \"File\",\n",
      "        \"MetricDefinitions\": [\n",
      "            {\n",
      "                \"Name\": \"training:epoch\",\n",
      "                \"Regex\": \"'epoch': (.*?)}\"\n",
      "            },\n",
      "            {\n",
      "                \"Name\": \"evaluation:loss\",\n",
      "                \"Regex\": \"'eval_loss': (.*?),\"\n",
      "            },\n",
      "            {\n",
      "                \"Name\": \"evaluation:accuracy\",\n",
      "                \"Regex\": \"'eval_accuracy': (.*?),\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"RoleArn\": \"arn:aws:iam::612233423258:role/service-role/AmazonSageMaker-ExecutionRole-20200324T172595\",\n",
      "    \"OutputDataConfig\": {\n",
      "        \"S3OutputPath\": \"s3://sm-transformers-datasets/models\"\n",
      "    },\n",
      "    \"TensorBoardOutputConfig\": {\n",
      "        \"S3OutputPath\": \"s3://sm-transformers-datasets/models\"\n",
      "    },\n",
      "    \"ResourceConfig\": {\n",
      "        \"InstanceCount\": 1,\n",
      "        \"InstanceType\": \"ml.g4dn.xlarge\",\n",
      "        \"VolumeSizeInGB\": 60\n",
      "    },\n",
      "    \"HyperParameters\": {\n",
      "        \"task_name\": \"classif\",\n",
      "        \"model_name\": \"bert-base-uncased\",\n",
      "        \"max_steps\": \"500\",\n",
      "        \"use_bbox\": \"false\",\n",
      "        \"per_device_train_batch_size\": \"10\",\n",
      "        \"per_device_eval_batch_size\": \"10\"\n",
      "    },\n",
      "    \"StoppingCondition\": {\n",
      "        \"MaxRuntimeInSeconds\": 86400,\n",
      "        \"MaxWaitTimeInSeconds\": 86400\n",
      "    },\n",
      "    \"EnableManagedSpotTraining\": true,\n",
      "    \"CheckpointConfig\": {\n",
      "        \"S3Uri\": \"s3://sm-transformers-datasets/models/sm-transformers-gpu-2021-02-17-09-55-35/checkpoints\"\n",
      "    },\n",
      "    \"InputDataConfig\": [\n",
      "        {\n",
      "            \"ChannelName\": \"train\",\n",
      "            \"DataSource\": {\n",
      "                \"S3DataSource\": {\n",
      "                    \"S3DataType\": \"S3Prefix\",\n",
      "                    \"S3Uri\": \"s3://sm-transformers-datasets/data/dataset_multiclass_500\",\n",
      "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
      "                }\n",
      "            },\n",
      "            \"ContentType\": \"text/plain\",\n",
      "            \"CompressionType\": \"None\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#cpu: ml.c4.4xlarge (16 cpus)\n",
    "\n",
    "common_training_params = \\\n",
    "{\n",
    "    \"TrainingJobName\": training_job_name,\n",
    "    \"AlgorithmSpecification\": {\n",
    "        \"TrainingImage\": image,\n",
    "        \"TrainingInputMode\": \"File\",\n",
    "        \"MetricDefinitions\" : metrics\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"OutputDataConfig\": {\n",
    "        \"S3OutputPath\": models_s3_uri\n",
    "    },\n",
    "    \"TensorBoardOutputConfig\": { \n",
    "      #\"LocalPath\": \"/opt/ml/output/tensorboard\", #default value is /opt/ml/output/tensorboard\n",
    "      \"S3OutputPath\": models_s3_uri\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        \"InstanceCount\": 1,   \n",
    "        \"InstanceType\": instance_type,\n",
    "        \"VolumeSizeInGB\": 60\n",
    "    },\n",
    "    \"HyperParameters\": hyperparameters,\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 86400,\n",
    "        \"MaxWaitTimeInSeconds\": 86400\n",
    "    },\n",
    "    \"EnableManagedSpotTraining\": True,\n",
    "    \"CheckpointConfig\": { \n",
    "      #\"LocalPath\": \"/opt/ml/checkpoints/\", #default value is /opt/ml/checkpoints/\n",
    "      \"S3Uri\": checkpoints_s3_uri\n",
    "   },\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": train_s3_uri,\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\" \n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"text/plain\",\n",
    "            \"CompressionType\": \"None\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(json.dumps(common_training_params, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a training job**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.5 ms, sys: 3.57 ms, total: 19.1 ms\n",
      "Wall time: 588 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'TrainingJobArn': 'arn:aws:sagemaker:eu-central-1:612233423258:training-job/sm-transformers-gpu-2021-02-17-09-55-35',\n",
       " 'ResponseMetadata': {'RequestId': '1228fb38-14f5-419d-a96b-57ce7d9a4f08',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '1228fb38-14f5-419d-a96b-57ce7d9a4f08',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '117',\n",
       "   'date': 'Wed, 17 Feb 2021 09:56:22 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sm.create_training_job(**common_training_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Training job ended with status: Completed\n",
      "CPU times: user 37.3 ms, sys: 49.1 ms, total: 86.4 ms\n",
      "Wall time: 713 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# monitor the training job\n",
    "status = sm.describe_training_job(TrainingJobName=training_job_name)['TrainingJobStatus']\n",
    "print(status)\n",
    "\n",
    "sm.get_waiter('training_job_completed_or_stopped').wait(TrainingJobName=training_job_name)\n",
    "status = sm.describe_training_job(TrainingJobName=training_job_name)['TrainingJobStatus']\n",
    "print(\"Training job ended with status: \" + status)\n",
    "if status == 'Failed':\n",
    "    message = sm.describe_training_job(TrainingJobName=training_job_name)['FailureReason']\n",
    "    print('Training failed with the following error: {}'.format(message))\n",
    "    raise Exception('Training job failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a model from Training Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training is finished, we can get the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sm-transformers-gpu-2021-02-17-09-55-35\n"
     ]
    }
   ],
   "source": [
    "#training_job_name = \"ner-bert-base-cased-gpu-2020-06-29-09-09-18\"\n",
    "print(training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that you can specify a different docker image for inference than the one used for the training. <br>\n",
    "In our case, if we want to use `CPU` instead of `GPU` resources in the inference step, we can set it explicitely by changing the image variable value. In our case: <br>\n",
    "`image_name = \"sm-transformers-cpu\"\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(account, region, image_name)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name : sm-transformers-gpu-2021-02-17-09-55-35-m\n",
      "model_data : s3://sm-transformers-datasets/models/sm-transformers-gpu-2021-02-17-09-55-35/output/model.tar.gz\n",
      "primary_container : {'Image': '612233423258.dkr.ecr.eu-central-1.amazonaws.com/sm-transformers-gpu:latest', 'ModelDataUrl': 's3://sm-transformers-datasets/models/sm-transformers-gpu-2021-02-17-09-55-35/output/model.tar.gz'}\n",
      "arn:aws:sagemaker:eu-central-1:612233423258:model/sm-transformers-gpu-2021-02-17-09-55-35-m\n"
     ]
    }
   ],
   "source": [
    "#### Uncomment if you want to use the CPU based image for Creating the model #####\n",
    "\n",
    "#image_name = \"sm-transformers-cpu\"\n",
    "#image = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(account, region, image_name)\n",
    "\n",
    "######################################  End ########################################\n",
    "\n",
    "\n",
    "# set the model name\n",
    "model_name = training_job_name + '-m'\n",
    "print(\"model_name : {}\".format(model_name))\n",
    "\n",
    "# get model artifacts location\n",
    "info = sm.describe_training_job(TrainingJobName=training_job_name)\n",
    "model_data = info['ModelArtifacts']['S3ModelArtifacts']\n",
    "print(\"model_data : {}\".format(model_data))\n",
    "    \n",
    "primary_container = {\n",
    "    'Image': image,\n",
    "    'ModelDataUrl': model_data\n",
    "}\n",
    "print(\"primary_container : {}\".format(primary_container))\n",
    "\n",
    "# Create model\n",
    "create_model_response = sm.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = primary_container)\n",
    "\n",
    "print(create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Serve the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Create a Sagemaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end_point_config_name: fig-name-sm-transformers-gpu-2021-02-17-09-55-35-m\n",
      "end_point_name: point-sm-transformers-gpu-2021-02-17-09-55-35-m\n"
     ]
    }
   ],
   "source": [
    "# set endpoint name\n",
    "end_point_config_name = \"end-point-config-name-{}\".format(model_name)\n",
    "end_point_config_name = end_point_config_name[max(len(end_point_config_name)-50,0):]\n",
    "end_point_name = \"point-{}\".format(model_name)\n",
    "end_point_name = end_point_name[max(len(end_point_name)-50,0):]\n",
    "print(\"end_point_config_name: {}\".format(end_point_config_name))\n",
    "print(\"end_point_name: {}\".format(end_point_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Config Arn: arn:aws:sagemaker:eu-central-1:612233423258:endpoint-config/fig-name-sm-transformers-gpu-2021-02-17-09-55-35-m\n"
     ]
    }
   ],
   "source": [
    "instance_type = \"ml.g4dn.2xlarge\" \n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName = end_point_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType': instance_type,\n",
    "        'InitialVariantWeight': 1,\n",
    "        'InitialInstanceCount': 1,\n",
    "        'ModelName': model_name,\n",
    "        'VariantName':'AllTraffic'}])\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enpoint Arn: arn:aws:sagemaker:eu-central-1:612233423258:endpoint/point-sm-transformers-gpu-2021-02-17-09-55-35-m\n",
      "Status: Creating\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=end_point_name,\n",
    "    EndpointConfigName=end_point_config_name)\n",
    "print(\"Enpoint Arn: {}\".format(create_endpoint_response['EndpointArn']))\n",
    "\n",
    "resp = sm.describe_endpoint(EndpointName=end_point_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:eu-central-1:612233423258:endpoint/point-sm-transformers-gpu-2021-02-17-09-55-35-m\n",
      "Status: InService\n"
     ]
    }
   ],
   "source": [
    "while status=='Creating':\n",
    "    time.sleep(60)\n",
    "    resp = sm.describe_endpoint(EndpointName=end_point_name)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp['EndpointArn'])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Invoke Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With **JSON** input and **JSON** output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"predictions\": [{\"pred\": \"tech\", \"proba\": 0.553991973400116}, {\"pred\": \"tech\", \"proba\": 0.3334293067455292}, {\"pred\": \"tech\", \"proba\": 0.5057740211486816}]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "data = {\"data\":[{'text': 'hello I am Alexis'}, \n",
    "                {'text': \"how are you\"},\n",
    "                {'text': \"are you doing fine??\"}]}\n",
    "query_body = json.dumps(data)\n",
    "\n",
    "\n",
    "runtime_client = boto3.client('runtime.sagemaker')\n",
    "response = runtime_client.invoke_endpoint(EndpointName = end_point_name,\n",
    "                                 ContentType = 'application/json', \n",
    "                                 #Accept='application/json', # by default it will return json\n",
    "                                 Body = query_body)\n",
    "result = response['Body'].read().decode('ascii')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With **JSONLINES** input and **JSONLINES** output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"predictions\": [{\"pred\": \"tech\", \"proba\": 0.553991973400116}, {\"pred\": \"tech\", \"proba\": 0.3334293067455292}, {\"pred\": \"tech\", \"proba\": 0.5057740211486816}]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "data = [{'text': 'hello I am Alexis'}, \n",
    "        {'text': \"how are you\"},\n",
    "        {'text': \"are you doing fine??\"}]\n",
    "query_body = query_body = \"\\n\".join([json.dumps(d) for d in data])\n",
    "\n",
    "runtime_client = boto3.client('runtime.sagemaker')\n",
    "response = runtime_client.invoke_endpoint(EndpointName = end_point_name,\n",
    "                                 ContentType = 'application/jsonlines', \n",
    "                                 Accept='application/jsonlines',\n",
    "                                 Body = query_body)\n",
    "result = response['Body'].read().decode('ascii')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Batch Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ContentType | Recommended SplitType |\n",
    "| --- | --- |\n",
    "| application/jsonlines | Line |\n",
    "| application/json | None |\n",
    "\n",
    "\n",
    "\n",
    "| Accept | Recommended AssembleWith |\n",
    "| --- | --- |\n",
    "| application/jsonlines | Line |\n",
    "| application/json | None |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With **JSONLInes** files as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform batch transform, you should have a s3 location (folder), containing one or mutiple .jsonl files:\n",
    "\n",
    "```\n",
    "s3_folder/\n",
    "    - file1.jsonl\n",
    "    - file2.jsonl\n",
    "    - file3.jsonl\n",
    "    ...\n",
    "```\n",
    "\n",
    "\n",
    "each file having the **jsonlines** format. That is each line being:\n",
    "\n",
    "\n",
    "```\n",
    "{'text': 'hello I am someone who wants a prediction'}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch input uri: s3://sm-transformers-datasets/batch_data/dataset_multiclass_500/jsonl\n",
      "batch output uri: s3://sm-transformers-datasets/batch_output/dataset_multiclass_500/jsonl_assembleNone\n"
     ]
    }
   ],
   "source": [
    "# Data location\n",
    "bucket_name = \"sm-transformers-datasets\" \n",
    "input_prefix = \"batch_data/dataset_multiclass_500/jsonl\"\n",
    "output_prefix = \"batch_output/dataset_multiclass_500/jsonl_assembleNone\"\n",
    "batch_input_s3_uri = \"s3://{}/{}\".format(bucket_name, input_prefix)\n",
    "batch_output_s3_uri = \"s3://{}/{}\".format(bucket_name, output_prefix)\n",
    "print(\"batch input uri: {}\".format(batch_input_s3_uri))\n",
    "print(\"batch output uri: {}\".format(batch_output_s3_uri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch job name: batch-2021-02-17-11-34-53\n"
     ]
    }
   ],
   "source": [
    "batch_job_name = \"batch-{}\".format(strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime()))\n",
    "batch_job_name = batch_job_name[max(len(batch_job_name)-50,0):]\n",
    "print(\"batch job name: {}\".format(batch_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_type_input = \"application/jsonlines\"\n",
    "max_payload_in_mb = 2\n",
    "batch_strategy = \"MultiRecord\" # MultiRecord | SingleRecord\n",
    "split_type = \"Line\" # None | Line\n",
    "# None : input data files are not split, and request payloads contain the entire contents of an input object\n",
    "# Line : depends on the values of the BatchStrategy and MaxPayloadInMB parameters. \n",
    "# If BatchStrategy = MultiRecord, Amazon SageMaker sends the maximum number of records in each request, \n",
    "# up to the MaxPayloadInMB limit. \n",
    "# If BatchStrategy = SingleRecord, Amazon SageMaker sends individual records in each request.\n",
    "content_type_output = \"application/jsonlines\"\n",
    "assemble_with = \"Line\" # None | Line\n",
    "# To concatenate the results in binary format, specify None. \n",
    "# To add a newline character at the end of every transformed record, specify Line.\n",
    "\n",
    "## NB: BatchStrategy=Multirecord + SplitType=Line + AssembleWith=Line will do a mapping between input files and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu: ml.p2.xlarge\n",
    "# cpu: ml.m4.xlarge\n",
    "\n",
    "request = \\\n",
    "{\n",
    "    \"TransformJobName\": batch_job_name,\n",
    "    \"ModelName\": model_name,\n",
    "    \"BatchStrategy\": batch_strategy,\n",
    "    \"MaxPayloadInMB\": max_payload_in_mb,\n",
    "    \"ModelClientConfig\": { \n",
    "      \"InvocationsTimeoutInSeconds\": 120\n",
    "       },\n",
    "    \"TransformInput\": {\n",
    "        \"DataSource\": {\n",
    "            \"S3DataSource\": {\n",
    "                \"S3DataType\": \"S3Prefix\",\n",
    "                \"S3Uri\": batch_input_s3_uri \n",
    "            }\n",
    "        },\n",
    "        \"ContentType\": content_type_input,\n",
    "        \"SplitType\": split_type,\n",
    "        \"CompressionType\": \"None\"\n",
    "    },\n",
    "    \"TransformOutput\": {\n",
    "        \"S3OutputPath\": batch_output_s3_uri,\n",
    "        \"Accept\": content_type_output,\n",
    "        \"AssembleWith\": assemble_with\n",
    "    },\n",
    "    \"TransformResources\": {\n",
    "            \"InstanceType\": \"ml.p2.xlarge\",\n",
    "            \"InstanceCount\": 1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TransformJobArn': 'arn:aws:sagemaker:eu-central-1:612233423258:transform-job/batch-2021-02-17-11-34-53',\n",
       " 'ResponseMetadata': {'RequestId': 'a86aef69-d95b-4d87-ba4b-01cee12a4cb7',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'a86aef69-d95b-4d87-ba4b-01cee12a4cb7',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '105',\n",
       "   'date': 'Wed, 17 Feb 2021 11:35:15 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.create_transform_job(**request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform job is still in status: InProgress\n",
      "Transform job is still in status: InProgress\n",
      "Transform job is still in status: InProgress\n",
      "Transform job is still in status: InProgress\n",
      "Transform job is still in status: InProgress\n",
      "Transform job is still in status: InProgress\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-f12197cfdf4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Transform job failed'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Transform job is still in status: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "while(True):\n",
    "    response = sm.describe_transform_job(TransformJobName=batch_job_name)\n",
    "    status = response['TransformJobStatus']\n",
    "    if  status == 'Completed':\n",
    "        print(\"Transform job ended with status: \" + status)\n",
    "        break\n",
    "    if status == 'Failed':\n",
    "        message = response['FailureReason']\n",
    "        print('Transform failed with the following error: {}'.format(message))\n",
    "        raise Exception('Transform job failed') \n",
    "    print(\"Transform job is still in status: \" + status)    \n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
