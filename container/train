#!/usr/bin/env python

import os
import json
import sys
import traceback
import logging
from glob import glob
from src.train_model import train_model


prefix = '/opt/ml/'
# input
input_data_path = os.path.join(prefix, 'input/data/train')
hyperparam_path = os.path.join(prefix, 'input/config/hyperparameters.json')
# temp
tmp_path = "/tmp/"
# output
checkpoints_path = os.path.join(prefix, 'checkpoints')
model_output_artifacts_path = os.path.join(prefix, 'model')
logging_path = os.path.join(prefix, 'output/tensorboard')

logger = logging.getLogger(__name__)

# Setup logging
logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(name)s -   %(message)s",
    datefmt="%m/%d/%Y %H:%M:%S",
    level=logging.INFO
)

def train():
    """
    This function will define training config based on default and SM hyperparameters
    and launch training / or resume based on a checkpoint
    """
    try:
        training_args = {
        # FROM https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments
        "output_dir": checkpoints_path ,
            # The output directory where the model predictions and checkpoints will be written. (not the final/best model)
        "overwrite_output_dir": True,
            # defaults to :obj:`False` # overwrite the content of the output directory. 
            #Use this to continue training if :obj:`output_dir` points to a checkpoint directory.
        "do_train": True , 
            # defaults to :obj:`False` # Whether to run training or not. This argument is not directly 
            #used by :class:`~transformers.Trainer`, it's intended to be used by your training/evaluation scripts instead. 
            #See the `example scripts <https://github.com/huggingface/transformers/tree/master/examples>`__ for more details.
        "do_eval": True, 
            # Whether to run evaluation on the validation set or not. 
            #Will be set to :obj:`True` if :obj:`evaluation_strategy` is different from :obj:`"no"`. 
            #This argument is not directly used by :class:`~transformers.Trainer`, it's intended to be 
            #used by your training/evaluation scripts instead. See the `example scripts 
            #<https://github.com/huggingface/transformers/tree/master/examples>`__ for more details.
        "do_predict": True , 
            # defaults to :obj:`False` # Whether to run predictions on the test set or not. 
            #This argument is not directly used by :class:`~transformers.Trainer`, it's intended to be used 
            #by your training/evaluation scripts instead. See the `example scripts 
            #<https://github.com/huggingface/transformers/tree/master/examples>`__ for more details.
        "evaluation_strategy": "steps" , 
            # defaults to :obj:`"no"` # The evaluation strategy to adopt during training. 
            #Possible values are: * :obj:`"no"`: No evaluation is done during training. * :obj:`"steps"`: 
            #Evaluation is done (and logged) every :obj:`eval_steps`. * :obj:`"epoch"`: Evaluation is done at 
            #the end of each epoch.
        #"prediction_loss_only": , 
            # defaults to `False` # When performing evaluation and generating predictions, 
            #only returns the loss.
        "per_device_train_batch_size": 5, 
            # defaults to 8 # The batch size per GPU/TPU core/CPU for training.
        "per_device_eval_batch_size": 5, 
            # defaults to 8 # The batch size per GPU/TPU core/CPU for evaluation.
        #"gradient_accumulation_steps": 1, 
            # defaults to 1 # Number of updates steps to accumulate the gradients for, 
            #before performing a backward/update pass.
            #.. warning::
            #    When using gradient accumulation, one step is counted as one step with backward pass. Therefore,
            #    logging, evaluation, save will be conducted every ``gradient_accumulation_steps * xxx_step`` training
            #    examples.
        #"eval_accumulation_steps": , 
            # :obj:`int`, `optional`  # Number of predictions steps to accumulate the 
            #output tensors for, before moving the results to the CPU. If left unset, the whole 
            #predictions are accumulated on GPU/TPU before being moved to the CPU (faster but requires more memory).
        #"learning_rate": , 
            # `optional`, defaults to 5e-5 # The initial learning rate for Adam.
        #"weight_decay": , 
            # defaults to 0 # The weight decay to apply (if not zero).
        #"adam_beta1": , 
            # defaults to 0.9 # The beta1 hyperparameter for the Adam optimizer.
        #"adam_beta2": , 
            # defaults to 0.999 # The beta2 hyperparameter for the Adam optimizer.
        #"adam_epsilon": , 
            # defaults to 1e-8 # The epsilon hyperparameter for the Adam optimizer.
        #"max_grad_norm": , 
            # defaults to 1.0 # Maximum gradient norm (for gradient clipping).
        #"num_train_epochs": 7, 
            # defaults to 3.0 # Total number of training epochs to perform 
            # (if not an integer, will perform the decimal part percents of the last epoch before stopping training).
        "max_steps": 50, 
            # defaults to -1 # If set to a positive number, the total number of training steps to perform. 
            #Overrides :obj:`num_train_epochs`.
        #"lr_scheduler_type": , 
            # defaults to :obj:`"linear"` # The scheduler type to use. See the documentation of 
            # :class:`~transformers.SchedulerType` for all possible values.
        #"warmup_steps": , 
            # defaults to 0 # Number of steps used for a linear warmup from 0 to :obj:`learning_rate`.
        "logging_dir": logging_path, 
            #  # `TensorBoard <https://www.tensorflow.org/tensorboard>`__ log directory. Will default to
            # `runs/**CURRENT_DATETIME_HOSTNAME**`.
        #"logging_first_step": , 
            # defaults to :obj:`False` # Whether to log and evaluate the first :obj:`global_step` or not.
        "logging_steps": 10, 
            # defaults to 500 # Number of update steps between two logs.
        "save_steps": 10, 
            # defaults to 500 # Number of updates steps before two checkpoint saves.
        #"save_total_limit": 3, 
            #  # If a value is passed, will limit the total amount of checkpoints. 
            # Deletes the older checkpoints in :obj:`output_dir`.
        #"no_cuda": , 
            # defaults to :obj:`False` # Whether to not use CUDA even when it is available or not.
        #"seed": , 
            # defaults to 42 # Random seed for initialization.
        #"fp16": , 
            # defaults to :obj:`False` # Whether to use 16-bit (mixed) precision training 
            # (through NVIDIA Apex) instead of 32-bit training.
        #"fp16_opt_level": , 
            # `optional`, defaults to 'O1' # For :obj:`fp16` training, Apex AMP optimization 
            # level selected in ['O0', 'O1', 'O2', and 'O3']. See details
            # on the `Apex documentation <https://nvidia.github.io/apex/amp.html>`__.
        #"local_rank": , 
            # defaults to -1 # Rank of the process during distributed training.
        #"tpu_num_cores": , 
            # optional # When training on TPU, the number of TPU cores 
            # (automatically passed by launcher script).
        #"debug": , 
            # defaults to :obj:`False` # When training on TPU, whether to print debug metrics or not.
        #"dataloader_drop_last": , 
            # defaults to :obj:`False` # Whether to drop the last incomplete 
            #batch (if the length of the dataset is not divisible by the batch size) or not.
        #"eval_steps": , 
            # optional # Number of update steps between two evaluations if 
            # :obj:`evaluation_strategy="steps"`. Will default to the same value as :obj:`logging_steps` if not set.
        #"dataloader_num_workers": , 
            # defaults to 0 # Number of subprocesses to use for data loading 
            # (PyTorch only). 0 means that the data will be loaded in the main process.
        #"past_index": , 
            # defaults to -1 # Some models like :doc:`TransformerXL <../model_doc/transformerxl>` 
            # or :doc`XLNet <../model_doc/xlnet>` can
            # make use of the past hidden states for their predictions. If this argument is set to a positive int, the
            # ``Trainer`` will use the corresponding output (usually index 2) as the past state and feed it to the model
            # at the next training step under the keyword argument ``mems``.
        #"run_name": , 
            # optional # A descriptor for the run. Typically used for `wandb <https://www.wandb.com/>`_ logging.
        #"disable_tqdm": , 
            # optional # Whether or not to disable the tqdm progress bars and table of metrics produced by
            # :class:`~transformers.notebook.NotebookTrainingTracker` in Jupyter Notebooks. Will default to :obj:`True`
            # if the logging level is set to warn or lower (default), :obj:`False` otherwise.
        "remove_unused_columns": True, 
            # defaults to :obj:`True` # If using :obj:`datasets.Dataset` datasets, 
            # whether or not to automatically remove the columns unused by the
            # model forward method. (Note that this behavior is not implemented for :class:`~transformers.TFTrainer` yet.)
        #"label_names": , 
            # optional # The list of keys in your dictionary of inputs that correspond to the labels.
            # Will eventually default to :obj:`["labels"]` except if the model used is one of the
            # :obj:`XxxForQuestionAnswering` in which case it will default to :obj:`["start_positions", "end_positions"]`.
        "load_best_model_at_end": True, 
            # defaults to :obj:`False` # Whether or not to load the best model found 
            # during training at the end of training. .. note:: When set to :obj:`True`, the parameters 
            # :obj:`save_steps` will be ignored and the model will be saved after each evaluation.
        "metric_for_best_model": "eval_loss", 
            # optional # Use in conjunction with :obj:`load_best_model_at_end` to specify 
            # the metric to use to compare two different models. Must be the name of a metric returned by 
            # the evaluation with or without the prefix :obj:`"eval_"`. Will default to :obj:`"loss"` if 
            # unspecified and :obj:`load_best_model_at_end=True` (to use the evaluation loss).
            # If you set this value, :obj:`greater_is_better` will default to :obj:`True`. Don't forget to set it to
            # :obj:`False` if your metric is better when lower.
        #"greater_is_better": , 
            # optional # Use in conjunction with :obj:`load_best_model_at_end` 
            # and :obj:`metric_for_best_model` to specify if better models should have a greater metric or not. 
            # Will default to:
            # - :obj:`True` if :obj:`metric_for_best_model` is set to a value that isn't :obj:`"loss"` or :obj:`"eval_loss"`.
            # - :obj:`False` if :obj:`metric_for_best_model` is not set, or set to :obj:`"loss"` or :obj:`"eval_loss"`.
        #"model_parallel": , 
            # defaults to :obj:`False` # If the model supports model parallelism and there 
            # is more than one device, whether to use model parallelism to distribute the model's modules across devices or not.
        #"ignore_skip_data": , 
            # defaults to :obj:`False` # When resuming training, whether or not to skip the 
            # epochs and batches to get the data loading at the same stage as in the previous training. If set to :obj:`True`, 
            # the training will begin faster (as that skipping step can take a long time) but will not 
            # yield the same results as the interrupted training would have.
        #"fp16_backend": , 
            # defaults to :obj:`"auto"` # The backend to use for mixed precision training. 
            # Must be one of :obj:`"auto"`, :obj:`"amp"` or :obj:`"apex"`. :obj:`"auto"` will use AMP or 
            # APEX depending on the PyTorch version detected, while the other choices will force the requested backend.
        #"sharded_ddp": , 
            # defaults to :obj:`False` # Use Sharded DDP training from `FairScale 
            # <https://github.com/facebookresearch/fairscale>`__ (in distributed training only). 
            # This is an experimental feature.
        #"label_smoothing_factor": , 
            # defaults to 0.0 # The label smoothing factor to use. Zero means no 
            # label smoothing, otherwise the underlying onehot-encoded labels are changed from 0s and 1s 
            # to :obj:`label_smoothing_factor/num_labels` and :obj:`1 - label_smoothing_factor + label_smoothing_factor/num_labels` respectively.
        #"adafactor": , 
            # defaults to :obj:`False` # Whether or not to use the :class:
            # `~transformers.Adafactor` optimizer instead of :class:`~transformers.AdamW`.
        }

        model_args = { 
        # DEFINED IN src/train_ner.py
        "model_name_or_path": "bert-base-uncased",
            # # Path to pretrained model or model identifier from huggingface.co/models
        #"config_name":,
            # Optional # Pretrained config name or path if not the same as model_name
        #"tokenizer_name":,
            # Optional # Pretrained tokenizer name or path if not the same as model_name
        "cache_dir": tmp_path, 
            # Optional # Where do you want to store the pretrained models downloaded from huggingface.co
        }

        data_args = {
        # DEFINED IN src/train_ner.py
        "task_name": "ner",
            # Optional # The name of the task (ner, pos...).
            #"dataset_name": "conll2003",# Optional # The name of the dataset to use (via the datasets library).
            #"dataset_config_name":,# Optional # The configuration name of the dataset to use (via the datasets library).
        "train_file": os.path.join(input_data_path, "train.csv"),
            # Optional # The input training data file (a csv or JSON file).
        "validation_file": os.path.join(input_data_path, "dev.csv"),
            # Optional # An optional input evaluation data file to evaluate on (a csv or JSON file).
        "test_file": os.path.join(input_data_path, "test.csv"),
            # Optional # An optional input test data file to predict on (a csv or JSON file).
            #"overwrite_cache":,# default=False # Overwrite the cached training and evaluation sets
            #"preprocessing_num_workers":,# Optional # The number of processes to use for the preprocessing.
        "pad_to_max_length": False,
            # default=False # Whether to pad all samples to model maximum sentence length. "
            # "If False, will pad the samples dynamically when batching to the maximum length in the batch. More "
            # "efficient on GPU but very bad for TPU."
            # Will be set to true if use_bbox is True (since the Datacollator won't work otherwise...)
        #"label_all_tokens":,
            # default=False # Whether to put the label for one word on all tokens of generated by that word or 
            # just on the one (in which case the other tokens will have a padding index).
        "use_bbox": False,
            # whether to use bbox features as input (for LayoutLM model)
        "sagemaker_output_path": model_output_artifacts_path
            # path to save the final model
        }

        # gather all default args to one dict
        dict_args = dict(training_args)
        dict_args.update(model_args)
        dict_args.update(data_args)

        # Load SageMaker Hyperparameters
        with open(hyperparam_path, 'r') as tc:
            sm_hyperparameters = json.load(tc)
        logger.info("SageMaker Hyperparameters passed: {}".format(json.dumps(sm_hyperparameters, sort_keys=True, indent=4)))

        # Overwrite args with hyperparameters
        MODELS_LIST = ["bert-base-uncased", "microsoft/layoutlm-base-uncased", 
               "allenai/longformer-base-4096", "roberta-base", "distilbert-base-multilingual-cased"]
        if "model_name" in sm_hyperparameters:
            if sm_hyperparameters["model_name"] in MODELS_LIST:
                dict_args["model_name_or_path"] = sm_hyperparameters["model_name"]
            else:
                raise ValueError("model_name should be in list:{}".format(MODELS_LIST))

        TASK_NAMES = ["ner", "classif", "regression"]
        if "task_name" in sm_hyperparameters and sm_hyperparameters["task_name"] in TASK_NAMES:
            dict_args["task_name"] = sm_hyperparameters["task_name"]
        else:
            raise ValueError("Please provide a task_name value among:{}".format(TASK_NAMES))

        if "max_steps" in sm_hyperparameters and sm_hyperparameters["max_steps"].isdigit():
            dict_args["max_steps"] = int(sm_hyperparameters["max_steps"])
        if "use_bbox" in sm_hyperparameters:
            dict_args["use_bbox"] = (sm_hyperparameters["use_bbox"].lower() == 'true')
        if "per_device_train_batch_size" in sm_hyperparameters and sm_hyperparameters["per_device_train_batch_size"].isdigit():
            dict_args["per_device_train_batch_size"] = int(sm_hyperparameters["per_device_train_batch_size"])
        if "per_device_eval_batch_size" in sm_hyperparameters and sm_hyperparameters["per_device_eval_batch_size"].isdigit():
            dict_args["per_device_eval_batch_size"] = int(sm_hyperparameters["per_device_eval_batch_size"])

        # set log step based on max_steps value (to not have too many checkpoints...)
        max_steps = dict_args["max_steps"]
        dict_args["logging_steps"] = max(10, int(max_steps/10))
        dict_args["save_steps"] = max(10, int(max_steps/10))

        # Check for available checkpoint (to resume training when spot instance used)
        def get_latest_checkpoint(checkpoint_paths):
            global_steps = [int(c.split("-")[-1].split("/")[0]) for c in checkpoint_paths]
            idx_latest = global_steps.index(max(global_steps))
            return checkpoint_paths[idx_latest]
        checkpoints = glob(os.path.join(dict_args["output_dir"], "checkpoint-*"))
        if checkpoints:
            last_checkpoint_path = get_latest_checkpoint(checkpoints)
            dict_args["model_name_or_path"] = last_checkpoint_path
            logger.info("Training will start from checkpoint path: {}".format(last_checkpoint_path))

        logger.info("Arguments passed to training: {}".format(json.dumps(dict_args, sort_keys=True, indent=4)))
        results = train_model(dict_args)
        logger.info("Training finished, results: {}".format(json.dumps(results, sort_keys=True, indent=4)))
        
    except Exception as e:
        trc = traceback.format_exc()
        with open(os.path.join(os.path.join(prefix, 'output'), 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        sys.exit(255)

if __name__ == '__main__':
    train()
    sys.exit(0)
